# Dashboard Streamlit - Interfaz Web para Scrapper Distribuido

## ğŸ¯ DescripciÃ³n General

El dashboard de Streamlit es una **interfaz web interactiva** que permite a los usuarios enviar tareas de scraping y visualizar resultados en tiempo real. ActÃºa como un **cliente especializado** que se conecta directamente al Router del sistema distribuido.

## ğŸ—ï¸ Arquitectura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    WebSocket     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Dashboard     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   Router Node   â”‚
â”‚   Streamlit     â”‚                  â”‚                 â”‚
â”‚                 â”‚  Mensajes JSON   â”‚                 â”‚
â”‚  - UI Web       â”‚                  â”‚ - Distribuye    â”‚
â”‚  - Cliente TCP  â”‚                  â”‚ - Coordina      â”‚
â”‚  - Thread Safe  â”‚                  â”‚ - Responde      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–²                                     â”‚
         â”‚                                     â–¼
         â”‚                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                            â”‚ Scrapper Nodes  â”‚
         â”‚                            â”‚                 â”‚
         â”‚                            â”‚ - Procesa URLs  â”‚
         â”‚                            â”‚ - Extrae datos  â”‚
         â”‚                            â”‚ - Retorna info  â”‚
         â”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Usuario    â”‚
   â”‚  (Browser)  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”Œ ConexiÃ³n con el Router

### 1. Establecimiento de ConexiÃ³n

```python
class StreamlitClient:
    def connect(self):
        # Crear conexiÃ³n TCP persistente
        self.connection = NodeConnection(
            node_type='router',
            ip=self.router_ip,
            port=self.router_port,
            on_message_callback=self._handle_message,
            sender_node_type='client',
            sender_id=self.client_id
        )
        
        # Enviar identificaciÃ³n como cliente
        identification_msg = {
            'type': 'IDENTIFICATION',
            'sender_id': self.client_id,
            'node_type': 'client',
            'data': {
                'is_boss': False,
                'is_temporary': False,
                'port': 0
            }
        }
```

### 2. CaracterÃ­sticas de la ConexiÃ³n

- **Protocolo**: TCP persistente con mensajes JSON
- **IdentificaciÃ³n**: Como nodo `client` no-boss
- **Callback**: FunciÃ³n para manejar respuestas asÃ­ncronas
- **Thread Safety**: Manejo seguro de concurrencia

## ğŸ“¨ Flujo de Mensajes

### 1. EnvÃ­o de Tarea de Scraping

```mermaid
sequenceDiagram
    participant U as Usuario
    participant D as Dashboard
    participant R as Router
    participant S as Scrapper
    
    U->>D: Ingresa URL y presiona "Scrapear"
    D->>D: Genera task_id Ãºnico
    D->>R: CLIENT_REQUEST {task_id, url}
    R->>R: Agrega tarea a cola
    R->>S: NEW_TASK {task_id, url}
    S->>S: Procesa URL (scraping)
    S->>R: TASK_RESULT {task_id, resultado}
    R->>D: TASK_RESULT {task_id, resultado}
    D->>D: Actualiza UI con resultado
    D->>U: Muestra resultado en dashboard
```

### 2. Tipos de Mensajes

#### Cliente â†’ Router (Peticiones)

```json
{
  "type": "CLIENT_REQUEST",
  "sender_id": "streamlit-abc123",
  "node_type": "client",
  "data": {
    "task_id": "uuid-Ãºnico",
    "url": "https://ejemplo.com",
    "timestamp": "2026-01-09T07:04:29.123Z"
  }
}
```

#### Router â†’ Cliente (Resultados)

```json
{
  "type": "TASK_RESULT",
  "sender_id": "router-principal", 
  "data": {
    "task_id": "uuid-Ãºnico",
    "success": true,
    "result": {
      "url": "https://ejemplo.com",
      "html_length": 16423,
      "links_count": 21,
      "links": ["https://link1.com", "https://link2.com"],
      "status": "success"
    }
  }
}
```

## ğŸ§µ Manejo de Threading y Estado

### Problema de Concurrencia

Streamlit ejecuta la interfaz en el **hilo principal**, pero los callbacks de red se ejecutan en **hilos separados**. El `session_state` NO es accesible desde otros hilos.

### SoluciÃ³n: Cola Thread-Safe

```python
# Cola global persistente entre reruns
if 'message_queue' not in st.session_state:
    st.session_state.message_queue = queue.Queue()

# Referencia global para callbacks
message_queue = st.session_state.message_queue

def _handle_task_result(self, message):
    """Ejecutado en hilo de red (NO puede usar session_state)"""
    result_data = {
        'task_id': data.get('task_id'),
        'success': data.get('success', False), 
        'result': data.get('result', {})
    }
    # Poner en cola thread-safe
    message_queue.put(result_data)

def process_pending_results():
    """Ejecutado en hilo principal (SÃ puede usar session_state)"""
    while not message_queue.empty():
        result_data = message_queue.get_nowait()
        # Procesar y actualizar session_state
        st.session_state.results.insert(0, result_entry)
```

### Flujo Thread-Safe

```
Hilo de Red              Cola Thread-Safe         Hilo Principal
     â”‚                        â”‚                        â”‚
     â–¼                        â”‚                        â”‚
[Recibe mensaje]              â”‚                        â”‚
     â”‚                        â”‚                        â”‚
     â–¼                        â”‚                        â”‚
[Procesa datos]               â”‚                        â”‚
     â”‚                        â”‚                        â”‚
     â–¼                        â–¼                        â”‚
[message_queue.put()]  â”€â”€â”€â”€â”€â”€[Queue]                   â”‚
                              â”‚                        â”‚
                              â”‚                        â–¼
                              â”‚               [process_pending_results()]
                              â”‚                        â”‚
                              â–¼                        â–¼
                          [queue.get()]      [Actualiza session_state]
                                                       â”‚
                                                       â–¼
                                              [UI se actualiza]
```

## ğŸ–¥ï¸ Componentes de la UI

### 1. Dashboard Principal

- **MÃ©tricas en tiempo real**: Total, exitosas, fallidas, pendientes
- **Auto-refresh**: Cada 2 segundos para updates dinÃ¡micos
- **Estado de conexiÃ³n**: Verde/Rojo con informaciÃ³n del router

### 2. Formulario de Scraping

```python
with st.form("scraping_form"):
    url = st.text_input("URL a scrapear")
    submit = st.form_submit_button("ğŸ•·ï¸ Scrapear")
    
    if submit and url:
        task_id = client.send_scraping_task(url)
        # Agregar a pending_tasks para tracking
        st.session_state.pending_tasks[task_id] = {
            'url': url,
            'status': 'pending',
            'timestamp': datetime.now().isoformat()
        }
```

### 3. VisualizaciÃ³n de Resultados

#### MÃ©tricas por Resultado
- **ğŸ“„ TamaÃ±o HTML**: Bytes del contenido descargado
- **ğŸ”— Enlaces**: Cantidad de links encontrados
- **ğŸ“Š Estado**: SUCCESS/ERROR

#### Enlaces Expandibles
```python
with st.expander(f"ğŸ”— Ver enlaces encontrados ({len(links)} mostrados)"):
    for idx, link in enumerate(links, 1):
        st.markdown(f"{idx}. `{link}`")
```

## ğŸ”„ Ciclo de Vida de una Tarea

### 1. IniciaciÃ³n (UI)
```python
# Usuario ingresa URL â†’ BotÃ³n submit
task_id = str(uuid.uuid4())
st.session_state.pending_tasks[task_id] = {...}
st.session_state.stats['pending'] += 1
```

### 2. EnvÃ­o (Red)
```python
# Mensaje enviado al router
connection.send_message({
    'type': 'CLIENT_REQUEST',
    'data': {'task_id': task_id, 'url': url}
})
```

### 3. Procesamiento (Externo)
- Router recibe y delega al Scrapper
- Scrapper procesa URL y extrae datos
- Scrapper envÃ­a resultado al Router

### 4. RecepciÃ³n (Callback Thread)
```python
def _handle_task_result(self, message):
    # Ejecutado en hilo de red
    message_queue.put(result_data)
```

### 5. ActualizaciÃ³n (Main Thread)
```python
def process_pending_results():
    # Procesar cola en hilo principal
    result_data = message_queue.get_nowait()
    
    # Mover de pending a results
    del st.session_state.pending_tasks[task_id]
    st.session_state.results.insert(0, result_entry)
    
    # Actualizar estadÃ­sticas
    st.session_state.stats['pending'] -= 1
    st.session_state.stats['successful'] += 1
```

### 6. VisualizaciÃ³n (UI)
- UI se refresca automÃ¡ticamente
- Nuevo resultado aparece en la lista
- MÃ©tricas se actualizan instantÃ¡neamente

## ğŸ›¡ï¸ Manejo de Errores

### Errores de ConexiÃ³n
```python
try:
    if client.connect():
        st.session_state.connected = True
        st.success("âœ… Conectado!")
    else:
        st.error("âŒ Error al conectar")
except Exception as e:
    st.error(f"Error de conexiÃ³n: {e}")
```

### Errores de Scraping
```python
if not result['success']:
    error_msg = result.get('error', 'Error desconocido')
    st.error(f"âŒ **Error:** {error_msg}")
```

## ğŸ›ï¸ ConfiguraciÃ³n

### Variables de Entorno
- `ROUTER_IP`: IP/hostname del router (default: 'router-node')
- `ROUTER_PORT`: Puerto del router (default: 7070)

### ParÃ¡metros de UI
- **Auto-refresh**: 2 segundos
- **Puerto Streamlit**: 8501
- **Modo headless**: Sin browser automÃ¡tico

## ğŸš€ Ventajas del Dashboard

1. **Tiempo Real**: Resultados aparecen inmediatamente
2. **Thread Safe**: Manejo correcto de concurrencia
3. **Persistente**: ConexiÃ³n TCP mantenida
4. **Visual**: MÃ©tricas y grÃ¡ficos intuitivos
5. **Escalable**: Soporta mÃºltiples tareas simultÃ¡neas
6. **Responsive**: Auto-refresh dinÃ¡mico

## ğŸ”§ Uso con Docker

```bash
# Construir imagen
sudo docker build -t streamlit-app -f streamlit_app/Dockerfile .

# Ejecutar contenedor
sudo docker run -d \
  --name streamlit-app \
  --network scrapper-network \
  -p 8501:8501 \
  -e ROUTER_IP=router-node \
  streamlit-app

# Acceder en navegador
http://localhost:8501
```

Este dashboard proporciona una interfaz moderna y eficiente para interactuar con el sistema distribuido de scraping, manteniendo la arquitectura orientada a mensajes y garantizando la consistencia de datos en entornos concurrentes.